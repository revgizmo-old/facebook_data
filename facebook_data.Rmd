---
title: "facebook_data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
```

## facebook_data

This is a project to work with downloaded facebook data

Much of this analysis is informed by Deeply Trivial's 6/14 post: [Working with Your Facebook Data in R](http://www.deeplytrivial.com/2018/06/working-with-your-facebook-data-in-r.html)

```{r load_posts}

library(jsonlite)

#check to make sure your_posts exists and path is correct
file.exists("C:/Users/Admin/Documents/data downloads/facebook-conorhealy/posts/your_posts.json") 

#load fb_posts
fb_posts <- fromJSON("C:/Users/Admin/Documents/data downloads/facebook-conorhealy/posts/your_posts.json")

```


## Exploring fb_posts

* fb-Posts is just a list wrapper of the status_updates dataframe

```{r}

fb_posts %>% 
  names()

fb_posts %>% 
  length()


```



## Identify Status Updates

```{r status_updates}

library(tidyverse)
library(anytime)
library(qdapRegex)
library(lubridate)


url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+" #to be used in extracting urls, should find a better way than having to use a precise REGEX like this


status_updates <- fb_posts$status_updates

if(exists("fb_posts")) {rm(fb_posts)}

status_updates <- status_updates %>% 
  mutate(timestamp = anytime(timestamp), #use the anytime package to convert timestamp to datetime
         min = min(timestamp), #use the lubridate package to parse timestamp 
         max = max(timestamp),
         year = year(timestamp),
         month = month(timestamp),
         day = day(timestamp),
         wday = wday(timestamp),
         hour = hour(timestamp),
         author = word(string = title, start = 1, end = 2, sep = fixed(" ")), #identify the author by using the word function to extract the first 2 words
         auto_author = author == "Conor Healy",
         link_urls = str_extract(attachments, url_pattern), #extract the urls of links
         post_text = rm_between(text.var = data, '"', '"', extract = TRUE),
         post_text = as.character(post_text),
         attachments_text = rm_between(text.var = attachments, '"', '"', extract = TRUE),
         attachments_text = as.character(attachments_text),
         tags_text = rm_between(text.var = tags, '"', '"', extract = TRUE),
         tags_text = as.character(tags_text)
         ) 

status_updates %>% 
  head()

status_updates <- status_updates %>% 
  mutate(
  )


status_updates %>% 
  filter(author != "Conor Healy") %>% 
  head()

status_updates %>% 
  count(author)


```

## The most frequent words used in my updates

* Removing some "happy", "birthday", "hbd", and some weird non-words to make the list more informative

```{r my_post_text}

library(tidytext)

#unnest the individual words in my satus updates and remove stopwords
my_post_text <- status_updates %>%
  unnest_tokens(word, post_text) %>%
  anti_join(stop_words)


counts <- my_post_text %>%
  filter(author == "Conor Healy") %>%
  drop_na(word) %>%
  count(word, sort = TRUE)

counts 

drop_words <- c("0080","0099","009c", "happy", "birthday", "hbd")

counts <- counts %>%
  filter(!word %in% drop_words)

counts


```

## Wordcloud

* Where is Heather?
* Where is Michael?
* *Erin is young enough that she should be on this list, but tiny

```{r wordcloud}

library(wordcloud)

counts %>%
  with(wordcloud(word, n, max.words = 50))


```




## Wordcloud with sentiment analysis



```{r wordcloud_sentiment}

library(reshape2)

counts %>%
  inner_join(get_sentiments("bing")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red","darkgreen"), max.words = 100)


```




## People who post the most to my timeline:

```{r authors}

#count by author
status_updates %>% 
  filter(author != "Conor Healy") %>% 
  count(author) %>% 
  arrange(-n)

#word counts
counts <- my_post_text %>%
  filter(author != "Conor Healy") %>%
  drop_na(word) %>%
  filter(!word %in% drop_words) %>% 
  count(word, sort = TRUE)


```



## Exploring status_updates over time

```{r}


status_updates %>% 
  ggplot(aes(x = year, fill = auto_author )) +
  geom_bar()
  

status_updates %>% 
  ggplot(aes(x = month, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = day, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = wday, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = hour, fill = auto_author )) +
  geom_bar() 

status_updates %>% 
  ggplot(aes(x = hour, fill = auto_author )) +
  geom_bar() +
  coord_polar(theta = "x", start = 0)




```


## Exploring tags & tags_text 



```{r}

status_updates %>% 
  head()

status_updates %>% 
 count(tags_text) %>% 
  arrange(-n)
    
tags <- status_updates$tags    

tags %>% 
  names()

tags[1]

str(tags)

length(tags)

str(tags[[1]])
str(tags[[8]])
str(tags[[942]])

status_updates$tags %>%
  flatten_chr() %>% 
  as_tibble() %>% 
  count(value) %>% 
  arrange(-n)



```

## Exploring data & post_text 



```{r}

status_updates %>% 
  head()

status_updates %>% 
 count(post_text) %>% 
  arrange(-n)
    

```

