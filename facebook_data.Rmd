---
title: "facebook_data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(anytime)
library(qdapRegex)
library(lubridate)

```

# facebook_data

This is a project to work with downloaded facebook data

Much of this analysis is informed by Deeply Trivial's 6/14 post: [Working with Your Facebook Data in R](http://www.deeplytrivial.com/2018/06/working-with-your-facebook-data-in-r.html)

I also got value from Kan Nishida's 3/16 post:[Working with JSON data in very simple way](https://blog.exploratory.io/working-with-json-data-in-very-simple-way-ad7ebcc0bb89)

* Started by using from JSON, but realized late in the analysis that th

```{r load_posts}

library(jsonlite)

#check to make sure your_posts exists and path is correct
file.exists("C:/Users/Admin/Documents/data downloads/facebook-conorhealy/posts/your_posts.json") 

#load fb_posts
fb_posts <- fromJSON("C:/Users/Admin/Documents/data downloads/facebook-conorhealy/posts/your_posts.json")

```


## Exploring fb_posts

* fb-Posts is just a list wrapper of the status_updates dataframe

```{r}

fb_posts %>% 
  names()

fb_posts %>% 
  length()


```



## Exploring status_updates

* status_updates is the real dataframe, but it contains a bunch of nested lists

```{r}

status_updates <- fb_posts$status_updates

if(exists("fb_posts")) {rm(fb_posts)} #don't need fb_posts anymore


status_updates %>% 
  class()

status_updates %>% 
  length()

status_updates %>% 
  names()

status_updates %>% 
  nrow()

status_updates[1,]




```



## Clean up timestamp

* timestamp is in some ridiculous format, so lets fix that.

```{r}

status_updates <- status_updates %>% 
  mutate(timestamp = anytime(timestamp), #use the anytime package to convert timestamp to datetime
         year = year(timestamp),
         month = month(timestamp),
         day_of_month = day(timestamp),
         hour = hour(timestamp),
         wday = wday(timestamp)
  ) 

status_updates[1,]

```



## Clean up data

* data contains something: presumably data.
    + when data != NULL, data contains a list of single observation dataframes, each with only one column: "post"
    + with a little bit of painful code, we can split out the post_text and clean up (remove) the data column


```{r}

data <- status_updates$data

length(data)

names(data)

str(data[1:5])

data[[1]] #each element is just a data frame with 1 observation and one column ("post")

data[[1]][[1]] # the posts are all character values

names(data) <- seq_along(data) #give data some names for easier reference

data2 <- unlist(data, recursive = FALSE)

length(data2)

data2[[1]]

status_updates <- status_updates %>% 
  mutate(post_text = rm_between(text.var = data, '"', '"', extract = TRUE), #there's probably a better way, but this pulls out the text in the column that's "quoted" and returns it in the same row, so preserves the original listing.  Because each data observation is either NULL or 1 data frame with 1 observation and 1 character column, we should be able to extract the text from that character column this way
         post_text = as.character(post_text), #need to turn the post_text list into a column
         data = NULL #clean up data, since we've extracted all data
  ) 

status_updates[1,]

rm(data, data2)

```



## Clean up title - create author, action, and action_nouns

* title contains multiple pieces of information in a single character column, so lets split it out a bit.
    + first 2 words are the name of the person, lets's call them "author"
    + separate the words after author into action and action_noun to separate how they did it from what they did

```{r}

status_updates %>% 
  select(title) 

#looks like the first 2 words are the name of the person (at least they are for me)

status_updates %>% 
  select(title) %>% 
  transmute(author = word(string = title, start = 1, end = 2, sep = fixed(" "))) %>% 
  count(author) %>% 
  arrange(-n)

#create a list of 3rd words to filter out weird people with >2 words in thier name
status_updates %>% 
  select(title) %>% 
  mutate(third_word = word(string = title, start = 3, end = 3, sep = fixed(" "))) %>% 
  count(third_word) %>% 
  filter(!third_word %in% c("Bozarth", "Mcgee")) %>% #filter out known deviants with 3+ words in their name
  transmute(third_words = paste0(third_word, collapse = '", "')) %>% 
  count(third_words)

third_words <- c("added", "is","posted", "shared", "updated", "was", "wrote")


#create the author field
status_updates <- status_updates %>% 
  mutate(third_word = word(string = title, start = 3, end = 3, sep = fixed(" ")),
         author_words_n = ifelse(third_word %in% third_words, 2, 3),
         author = word(string = title, start = 1, end = author_words_n, sep = fixed(" ")),
         auto_author = author == "Conor Healy"
         )

status_updates %>% 
  filter(author != "Conor Healy") %>% 
  nrow()


status_updates %>% 
  count(author) %>% 
  arrange(-n)


#create the action field
status_updates <- status_updates %>% 
  mutate(action = word(string = title, start = author_words_n + 1, end = author_words_n + 1, sep = fixed(" ")),
         action_n = case_when(
           action %in% c("added", "is", "recommends", "updated", "was") ~ 1,
           action %in% c("posted", "wrote") ~ 2,
           action %in% c("shared") ~ 3,
           TRUE ~ 1
         ),
         action_detail = word(string = title, start = author_words_n + 1, end = author_words_n + action_n, sep = fixed(" ")),
         action_detail = str_replace(action_detail, "\\.", ""),
         title_n = str_count(title, " "), 
         action_noun = word(string = title, start = author_words_n + action_n + 1, end = title_n + 1, sep = fixed(" ")),
         author_words_n = NULL,
         action_n = NULL,
         title_n = NULL,
         third_word = NULL
  )

status_updates %>% 
count(action, action_detail) %>% 
  arrange(action, -n)

rm(third_words)

status_updates[1,]

```



## Clean up attachments

* attachments contains something: presumably data.
    + when attachments != NULL, attachments contains a list of dataframes, each with complicated breakdowns of media, comments, and other context that doesn't appear easy to wrangle.
    + I can extract link urls using the REGEX cribbed from Deeply Trivial's 6/14 post: [Working with Your Facebook Data in R](http://www.deeplytrivial.com/2018/06/working-with-your-facebook-data-in-r.html)
    + I think I'm going to leave this alone for now

```{r}

url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+" #to be used in extracting urls, should find a better way than having to use a precise REGEX like this.  REGEX cribbed from Deeply Trivial's 6/14 post: [Working with Your Facebook Data in R](http://www.deeplytrivial.com/2018/06/working-with-your-facebook-data-in-r.html)
                                                 
attachments <- status_updates$attachments

length(attachments)

names(attachments)

names(attachments) <- seq_along(attachments) #give attachments some names for easier reference

str(attachments[1:5])

status_updates <- status_updates %>% 
  mutate(link_urls = str_extract(attachments, url_pattern), #extract the urls of links
         link_domain = word(string = link_urls, start = 3, end = 3, sep = fixed("/")), #get the full domain name
         link_domain = word(string = link_domain, start = -2, end = -1, sep = fixed(".")) #remove the sub-domain(s)
  )

status_updates %>% 
  select(link_urls, link_domain) %>% 
  count(link_domain) %>% 
  arrange(-n)

rm(attachments)

status_updates[1,]

```





## Clean up tags

* tags contains something: presumably data.
    + when tags != NULL, tags contains a list of charater vectors, each with names of people
    + I need to build this out more
    


```{r tags}

tags <- status_updates$tags

length(tags)

names(tags)

names(tags) <- seq_along(tags) #give attachments some names for easier reference

status_updates <- tibble::rowid_to_column(status_updates, "ID")

str(tags[1:10])

status_updates <- status_updates %>% 
  mutate(tags_text = rm_between(text.var = tags, '"', '"', extract = TRUE), #there's probably a better way, but this pulls out the text in the column that's "quoted" and returns it in the same row, so preserves the original listing.  Because each data observation is either NULL or 1 data frame with 1 observation and 1 character column, we should be able to extract the text from that character column this way
         tags_text = as.character(tags_text)
  )

status_updates %>% 
  count(tags_text) %>% 
  arrange(-n)

tags_flat <- flatten_chr(tags)

tags_flat %>% 
  as_tibble() %>% 
  count(value) %>% 
  arrange(-n)

rm(tags)

status_updates[1,]
```

## The most frequent words used in my updates

* Removing some "happy", "birthday", "hbd", and some weird non-words to make the list more informative

```{r my_post_text}

library(tidytext)

#unnest the individual words in my satus updates and remove stopwords
my_post_text <- status_updates %>%
  unnest_tokens(word, post_text) %>%
  anti_join(stop_words)


counts <- my_post_text %>%
  filter(author == "Conor Healy") %>%
  drop_na(word) %>%
  count(word, sort = TRUE)

counts 

drop_words <- c("0080","0099","009c", "happy", "birthday", "hbd")

counts <- counts %>%
  filter(!word %in% drop_words)

counts


```

## Wordcloud

* Where is Heather?
* Where is Michael?
* *Erin is young enough that she should be on this list, but tiny

```{r wordcloud}

library(wordcloud)

counts %>%
  with(wordcloud(word, n, max.words = 50))


```




## Wordcloud with sentiment analysis



```{r wordcloud_sentiment}

library(reshape2)

counts %>%
  inner_join(get_sentiments("bing")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red","darkgreen"), max.words = 100)


```




## People who post the most to my timeline:

```{r authors}

#count by author
status_updates %>% 
  filter(author != "Conor Healy") %>% 
  count(author) %>% 
  arrange(-n)

#word counts
counts <- my_post_text %>%
  filter(author != "Conor Healy") %>%
  drop_na(word) %>%
  filter(!word %in% drop_words) %>% 
  count(word, sort = TRUE)


```



## Exploring status_updates over time

```{r}


status_updates %>% 
  ggplot(aes(x = year, fill = auto_author )) +
  geom_bar()
  

status_updates %>% 
  ggplot(aes(x = month, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = day_of_month, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = wday, fill = auto_author )) +
  geom_bar()


status_updates %>% 
  ggplot(aes(x = hour, fill = auto_author )) +
  geom_bar() 

status_updates %>% 
  ggplot(aes(x = hour, fill = auto_author )) +
  geom_bar() +
  coord_polar(theta = "x", start = 0)




```


## Exploring tags & tags_text 



```{r}

status_updates %>% 
  head()

status_updates %>% 
 count(tags_text) %>% 
  arrange(-n)
    
tags <- status_updates$tags    

tags %>% 
  names()

tags[1]

str(tags)

length(tags)

str(tags[[1]])
str(tags[[8]])
str(tags[[942]])

status_updates$tags %>%
  flatten_chr() %>% 
  as_tibble() %>% 
  count(value) %>% 
  arrange(-n)



```

## Exploring data & post_text 



```{r}

status_updates %>% 
  head()

status_updates %>% 
 count(post_text) %>% 
  arrange(-n)
    
<<<<<<< HEAD

```


some good stuff from [gluc](https://gist.github.com/gluc)/[Desc_JSON_to_df.md]
(https://gist.github.com/gluc/5f780246d57897b57c6b)

```{r}

# #devtools::install_github("gluc/data.tree")
# library(data.tree)
# library(jsonlite)
# library(magrittr)
# # reposLoL <- fromJSON("https://api.github.com/users/hadley/repos", simplifyDataFrame = FALSE)
# 
# reposLoL <- fromJSON("C:/Users/Admin/Documents/data downloads/facebook-conorhealy/posts/your_posts.json", simplifyDataFrame = FALSE)
# 
# library(data.tree)
# repos <- as.Node(reposLoL)
# print(repos, "id", "login")
# 
# #convert this to a data.frame
# reposdf <- repos %>% ToDataFrameTable(ownerId = "id", 
#                                   "login", 
#                                   repoName = function(x) x$parent$name, #relative to the leaf
#                                   fullName = "full_name", #unambiguous values are inherited from ancestors
#                                   repoId = function(x) x$parent$id,
#                                   "fork", 
#                                   "type")
# 
# reposdf
# 
# #Now a somewhat more advanced example: In addition, let's download 
# #contributors to each repo and store them in the same tree
# #NOTE: you can only call 50 unauthenticated api requests per hour,
# #so you can run this only once
# 
# #Get can also call functions! Here, we are not so much interested in
# #the result, but more as some sort of mapply
# repos$Get(function(x) x$AddChild("contributors"), filterFun = function(x) x$level == 2)
# getContribs <- function(x) {
#   contributors <- fromJSON(x$contributors_url, simplifyDataFrame = FALSE)
#   for(c in contributors) c %>% as.Node(nodeName = c$login) %>% x$Find("contributors")$AddChildNode()
#   return(length(contributors))
# }
# 
# repos$Get(getContribs, filterFun = function(x) x$level == 2)
# 
# #optional: print some information about our structure
# print(repos, "login", "id", "contributions")
# repos$fieldsAll
# #fields/attributes on contributors:
# repos$Find("crantastic", "owner", "hadley")$fields
# 
# #convert it to a table (all attributes are relative to the leaves, i.e. contributors)
# contributorsdf <- repos %>% ToDataFrameTable(contributorId = "id",
#                                              "login", 
#                                              isOwner = function(x) x$login == x$parent$parent$Find("owner")$login,
#                                              ownerName = function(x) x$parent$parent$Find("owner")$login,
#                                              repoName = function(x) x$parent$parent$name,
#                                              repoId = function(x) x$parent$parent$id,
#                                              "fork", 
#                                              "type",
#                                              "contributions",
#                                              filterFun = function(x) x$name != "owner"
# )

=======
>>>>>>> 6abfc53ff10c6dfc89c463cae37b03398a798253

```

